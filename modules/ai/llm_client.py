"""
LLM Client - Ð¾Ð±ÐµÑ€Ñ‚ÐºÐ° Ð½Ð°Ð´ DeepSeek/OpenAI API
"""
import os
import json
import logging
import requests
from typing import Optional, Dict, Any

logger = logging.getLogger(__name__)


class LLMClient:
    """
    ÐšÐ»Ð¸ÐµÐ½Ñ‚ Ð´Ð»Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ LLM API (DeepSeek, OpenAI)
    """
    
    # API endpoints
    PROVIDERS = {
        'deepseek': {
            'base_url': 'https://api.deepseek.com/v1',
            'default_model': 'deepseek-chat'
        },
        'openai': {
            'base_url': 'https://api.openai.com/v1',
            'default_model': 'gpt-4o-mini'
        }
    }
    
    def __init__(self, provider: str = None, api_key: str = None, model: str = None):
        """
        Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°
        
        Args:
            provider: 'deepseek' Ð¸Ð»Ð¸ 'openai' (Ð¸Ð· .env AI_PROVIDER)
            api_key: API ÐºÐ»ÑŽÑ‡ (Ð¸Ð· .env AI_API_KEY)
            model: ÐœÐ¾Ð´ÐµÐ»ÑŒ (Ð¸Ð· .env AI_MODEL)
        """
        self.provider = provider or os.getenv('AI_PROVIDER', 'deepseek')
        self.api_key = api_key or os.getenv('AI_API_KEY')
        
        if not self.api_key:
            raise ValueError("AI_API_KEY not set in environment")
        
        provider_config = self.PROVIDERS.get(self.provider)
        if not provider_config:
            raise ValueError(f"Unknown provider: {self.provider}. Use 'deepseek' or 'openai'")
        
        self.base_url = provider_config['base_url']
        self.model = model or os.getenv('AI_MODEL', provider_config['default_model'])
        self.timeout = int(os.getenv('AI_TIMEOUT', 30))
        
        logger.info(f"ðŸ¤– LLMClient initialized: {self.provider}/{self.model}")
    
    def ask(self, system_prompt: str, user_prompt: str = None, temperature: float = 0.7) -> str:
        """
        ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÑ‚ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ðº LLM Ð¸ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚
        
        Args:
            system_prompt: Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ (Ñ€Ð¾Ð»ÑŒ, ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚, Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¸)
            user_prompt: ÐŸÐ¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÑÐºÐ¸Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾)
            temperature: Ð¢ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð° (0.0-1.0)
            
        Returns:
            str: Ð¢ÐµÐºÑÑ‚Ð¾Ð²Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ Ð¾Ñ‚ Ð¼Ð¾Ð´ÐµÐ»Ð¸
        """
        messages = [{"role": "system", "content": system_prompt}]
        
        if user_prompt:
            messages.append({"role": "user", "content": user_prompt})
        
        return self._send_request(messages, temperature=temperature)
    
    def ask_json(self, system_prompt: str, user_prompt: str = None, temperature: float = 0.3) -> Dict[str, Any]:
        """
        ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÑ‚ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ðº LLM Ð¸ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ JSON Ð¾Ñ‚Ð²ÐµÑ‚
        
        Args:
            system_prompt: Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ (Ð´Ð¾Ð»Ð¶ÐµÐ½ Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ñ‚ÑŒ JSON)
            user_prompt: ÐŸÐ¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÑÐºÐ¸Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ (Ð¾Ð¿Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾)
            temperature: Ð¢ÐµÐ¼Ð¿ÐµÑ€Ð°Ñ‚ÑƒÑ€Ð° (Ð½Ð¸Ð¶Ðµ Ð´Ð»Ñ ÐºÐ¾Ð½ÑÐ¸ÑÑ‚ÐµÐ½Ñ‚Ð½Ð¾ÑÑ‚Ð¸)
            
        Returns:
            dict: Ð Ð°ÑÐ¿Ð°Ñ€ÑÐµÐ½Ð½Ñ‹Ð¹ JSON Ð¾Ñ‚Ð²ÐµÑ‚
        """
        messages = [{"role": "system", "content": system_prompt}]
        
        if user_prompt:
            messages.append({"role": "user", "content": user_prompt})
        
        response_text = self._send_request(
            messages, 
            temperature=temperature,
            response_format={"type": "json_object"}
        )
        
        # ÐŸÐ°Ñ€ÑÐ¸Ð¼ JSON
        try:
            return json.loads(response_text)
        except json.JSONDecodeError as e:
            logger.error(f"âŒ Failed to parse JSON response: {e}")
            logger.error(f"Raw response: {response_text[:500]}")
            raise ValueError(f"LLM returned invalid JSON: {e}")
    
    def _send_request(
        self, 
        messages: list, 
        temperature: float = 0.7,
        response_format: dict = None
    ) -> str:
        """
        ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÑ‚ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ðº API
        """
        url = f"{self.base_url}/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": 4000
        }
        
        # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ response_format ÐµÑÐ»Ð¸ ÑƒÐºÐ°Ð·Ð°Ð½
        if response_format:
            payload["response_format"] = response_format
        
        try:
            logger.debug(f"ðŸ“¤ Sending request to {self.provider}...")
            
            response = requests.post(
                url,
                headers=headers,
                json=payload,
                timeout=self.timeout
            )
            
            response.raise_for_status()
            
            data = response.json()
            
            # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²
            usage = data.get('usage', {})
            logger.info(f"ðŸ“Š Tokens used: {usage.get('total_tokens', '?')} "
                       f"(prompt: {usage.get('prompt_tokens', '?')}, "
                       f"completion: {usage.get('completion_tokens', '?')})")
            
            # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð¾Ñ‚Ð²ÐµÑ‚
            content = data['choices'][0]['message']['content']
            
            return content.strip()
            
        except requests.exceptions.Timeout:
            logger.error(f"â±ï¸ API timeout after {self.timeout}s")
            raise TimeoutError(f"LLM API timeout after {self.timeout}s")
            
        except requests.exceptions.HTTPError as e:
            logger.error(f"âŒ HTTP error: {e}")
            if hasattr(e, 'response') and e.response:
                logger.error(f"Response: {e.response.text[:500]}")
            raise
            
        except Exception as e:
            logger.error(f"âŒ API error: {e}")
            raise


# Singleton instance Ð´Ð»Ñ ÑƒÐ´Ð¾Ð±ÑÑ‚Ð²Ð°
_client: Optional[LLMClient] = None


def get_llm_client() -> LLMClient:
    """
    Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ singleton instance LLMClient
    """
    global _client
    
    if _client is None:
        _client = LLMClient()
    
    return _client
